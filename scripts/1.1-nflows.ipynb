{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def2cbb-c175-41a7-b7ba-9e2b440c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle = 1\n",
    "seeding_dataset = 'data/ChemBL-35-cleaned.csv' # initialized with 50k-ChemBL.csv\n",
    "\n",
    "trainingset_path  = 'data/'+str(cycle)+'-training_set_org.csv'  \n",
    "charset_path= 'data/1-0.001-inp.h5'\n",
    "latent_dataset_path = 'data/'+str(cycle)+'-training_set_latent_space.csv'\n",
    "PATH = \"checkpoint_239.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49790172-1639-425f-83cd-968e204d68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from group_selfies import (\n",
    "    fragment_mols, \n",
    "    Group, \n",
    "    MolecularGraph, \n",
    "    GroupGrammar, \n",
    "    group_encoder\n",
    ")\n",
    "\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "import IPython.display # from ... import display\n",
    "from test_utils import *\n",
    "from rdkit import RDLogger\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "import os\n",
    "import sys\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit.Chem import QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52497b9-2d30-4e28-81ba-ecdc223ffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "def one_hot_array(i, n):\n",
    "    return map(int, [ix == i for ix in xrange(n)])\n",
    "\n",
    "def one_hot_index(vec, charset):\n",
    "    return map(charset.index, vec)\n",
    "\n",
    "def from_one_hot_array(vec):\n",
    "    oh = np.where(vec == 1)\n",
    "    if oh[0].shape == (0, ):\n",
    "        return None\n",
    "    return int(oh[0][0])\n",
    "\n",
    "def decode_smiles_from_indexes(vec, charset):\n",
    "    # Ensure that each element in 'vec' is a string (not numpy.bytes_)\n",
    "    return \"\".join(map(lambda x: str(charset[x], 'utf-8') if isinstance(charset[x], bytes) else charset[x], vec)).strip()\n",
    "\n",
    "def load_dataset(filename, split = True):\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "    if split:\n",
    "        data_train = h5f['data_train'][:]\n",
    "    else:\n",
    "        data_train = None\n",
    "    data_test = h5f['data_test'][:]\n",
    "    charset =  h5f['charset'][:]\n",
    "    h5f.close()\n",
    "    if split:\n",
    "        return (data_train, data_test, charset)\n",
    "    else:\n",
    "        return (data_test, charset)\n",
    "\n",
    "class MolecularVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(120, 9, kernel_size=9)\n",
    "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "        self.linear_0 = nn.Linear(280, 435) # changed from 70 to 280 to reflect the change of charset size\n",
    "        self.linear_1 = nn.Linear(435, 292)\n",
    "        self.linear_2 = nn.Linear(435, 292)\n",
    "        \n",
    "        self.linear_3 = nn.Linear(292, 292)\n",
    "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(501, 54) # changed this output from 33 to 54 to reflect the larger charset size\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "        return self.linear_1(x), self.linear_2(x)\n",
    "\n",
    "    def sampling(self, z_mean, z_logvar):\n",
    "        epsilon = 1e-2 * torch.randn_like(z_logvar)\n",
    "        return torch.exp(0.5 * z_logvar) * epsilon + z_mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)\n",
    "        output, hn = self.gru(z)\n",
    "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.sampling(z_mean, z_logvar)\n",
    "        return self.decode(z), z_mean, z_logvar\n",
    "\n",
    "\n",
    "def load_dataset_chunked(filename, split=True, batch_size=10000):\n",
    "    # Open the HDF5 file explicitly\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "\n",
    "    # Memory-mapping the data (this avoids loading the entire dataset into memory at once)\n",
    "    data_test = np.array(h5f['data_test'], dtype='float32', copy=False)\n",
    "    \n",
    "    # Handle charset as strings directly\n",
    "    charset = h5f['charset']\n",
    "    if charset.dtype.kind in {'S', 'O'}:  # If it's a string or object type\n",
    "        charset = [x.decode('utf-8') if isinstance(x, bytes) else x for x in charset]  # Decode bytes if needed\n",
    "    else:\n",
    "        charset = np.array(charset, dtype='float32', copy=False)\n",
    "    \n",
    "    if split:\n",
    "        # Instead of loading the entire data_train, we'll iterate in chunks\n",
    "        data_train = h5f['data_train']\n",
    "        total_samples = data_train.shape[0]\n",
    "        \n",
    "        # Define the generator that reads data in chunks\n",
    "        def data_batch_generator():\n",
    "            \"\"\"Generator to load data in batches.\"\"\"\n",
    "            for i in range(0, total_samples, batch_size):\n",
    "                batch = data_train[i:i+batch_size]  # Read a batch from disk\n",
    "                yield batch\n",
    "\n",
    "        # Return the generator, data_test, and charset\n",
    "        return (data_batch_generator(), data_test, charset)\n",
    "    else:\n",
    "        # If not splitting, return data_test and charset only\n",
    "        return (data_test, charset)\n",
    "    \n",
    "    # Don't forget to close the file manually when done\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a3889-2895-45fb-90a6-5f5ee37ee453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MolecularVAE().to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class dotdict(dict): \n",
    "  __getattr__ = dict.get\n",
    "  __setattr__ = dict.__setitem__\n",
    "  __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c60fcd-e481-4183-94a6-a5f14da4f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_losses_p = checkpoint['train_losses_p']\n",
    "val_losses_p = checkpoint['val_losses_p']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a45ac-9021-4a1f-9563-3b5e110293df",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, charset = load_dataset(charset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495782ec-bc3a-4086-a93e-21d5d6322b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_org = pd.read_csv(trainingset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446673c-ea1c-4f9c-81ac-feceb595395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_org.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a0ea339-f7ec-4d72-8747-15082473a905",
   "metadata": {},
   "source": [
    "qed_mean = training_set_org.loc[:, 'QED'].mean()\n",
    "sa_mean  = training_set_org.loc[:, 'SA_score'].mean()\n",
    "nan_values = {\"QED\": qed_mean, \"SA_score\": sa_mean}\n",
    "training_set_org = training_set_org.fillna(value=nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2f3a8-4218-442d-ba84-57dee21e3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = pd.read_csv(latent_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6122b-25ee-437c-b53e-d642dbb30895",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.float32(latent_data)\n",
    "y = np.float32(training_set_org[['QED']])#, 'SA_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d26bfb-ab52-49e9-a838-4b36dad03279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, representation, values):\n",
    "        'Initialization'\n",
    "        self.lines = representation\n",
    "        self.values = values\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.values.shape[0]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        y = y_train[index,:]\n",
    "        x = x_train[index,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712188b-4533-4641-afc3-e184159a1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_s1 = training_set_org.sort_values(by=['QED']).dropna(subset=['QED']).drop_duplicates(subset=['QED']).tail(100)\n",
    "#    data_s2 = training_set_org.sort_values(by=['SA_score'], ascending=True).dropna(subset=['SA_score']).drop_duplicates(subset=['SA_score']).head(100)\n",
    "    data_s1 = data_s1['QED'].reset_index(drop=True)\n",
    "#    data_s2 = data_s2['SA_score'].reset_index(drop=True)\n",
    "#    y_s = np.float32(pd.concat([data_s1, data_s2], axis=1, ignore_index=True))\n",
    "    y_s = np.float32(data_s1)\n",
    "    y_s = torch.tensor(y_s).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b35c0-8325-46c1-97af-151fa155c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    args.batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec69e9-135e-4737-8b96-e9c5a698ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_ratio = 0.80\n",
    "    validation_ratio = 0.20\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "    # Data container:\n",
    "    training_set = Dataset(y_train, x_train)\n",
    "    validation_set = Dataset(y_val, x_val)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=training_set,  batch_size=args.batch_size,shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=validation_set, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    print('train: ', len(training_set), ', test: ' , len(validation_set), 'sample len: ', len(y_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56c562-2a7d-447f-84e2-370c979129d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    flow_train_losses = []\n",
    "    flow_val_losses = []\n",
    "\n",
    "    #num_shared_embedding = 50\n",
    "    num_layers = 4\n",
    "    hidden_features = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd200f2-27e5-42a6-a2a2-6e056c2e8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bayesiains/nflows/blob/master/examples/moons.ipynb\n",
    "from nflows import transforms, distributions, flows\n",
    "\n",
    "base_dist = distributions.StandardNormal(shape=[x_train.shape[1]])#, context=[y_train.shape[1]])\n",
    "transform = []\n",
    "for _ in range(num_layers):\n",
    "    transform.append(transforms.autoregressive.MaskedAffineAutoregressiveTransform(features=x_train.shape[1], \n",
    "                                                                     hidden_features=hidden_features))\n",
    "    transform.append(transforms.permutations.RandomPermutation(features=x_train.shape[1]))\n",
    "\n",
    "transform_list = transforms.base.CompositeTransform(transform)\n",
    "\n",
    "flow = flows.base.Flow(transform_list, base_dist)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72771d76-9fad-4b56-ae68-42a702279abc",
   "metadata": {},
   "source": [
    "# In case of restart:\n",
    "PATH = \"c1.pth\"\n",
    "\n",
    "checkpoint_flow = torch.load(PATH)#, weights_only=True)\n",
    "flow.load_state_dict(checkpoint_flow['model_state_dict'])\n",
    "epoch = checkpoint_flow['epoch']\n",
    "flow_train_losses = checkpoint_flow['train_losses_p']\n",
    "flow_val_losses = checkpoint_flow['val_losses_p']\n",
    "\n",
    "flow.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29ba13-ae1a-4653-a8c7-2617207409e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    args.learning_rate = 1.e-3\n",
    "    args.n_epochs = 10000\n",
    "    last_epoch = len(flow_train_losses)\n",
    "    args.log_interval2 = 500 # (args.n_epochs-last_epoch)/100\n",
    "\n",
    "    # Size and network parameters\n",
    "    pytorch_total_params_grad = sum(p.numel() for p in flow.parameters() if p.requires_grad)\n",
    "    print('Total params to optimize:', pytorch_total_params_grad)\n",
    "\n",
    "    optimizer = optim.Adam(flow.parameters(),\n",
    "                           lr=args.learning_rate)#, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-4, T_max=args.n_epochs)\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[args.n_epochs/5*2,args.n_epochs/5*4], gamma=0.1)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    flow = flow.cuda()\n",
    "    \n",
    "    time0 = time.time()\n",
    "\n",
    "    for epoch in range(last_epoch +1, args.n_epochs + 1):\n",
    "        flow.train()\n",
    "        flow_train_loss = 0\n",
    "        for batch_idx, (params, datas) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            params = params.to(device)\n",
    "            #datas = datas.reshape(-1, 1).to(device)\n",
    "            #print('shapes', params.shape, datas.shape)\n",
    "            loss = -flow.log_prob(inputs=params).mean() #, context=datas).mean()\n",
    "            #loss = -flow.log_prob(inputs=params, context=datas).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            flow_train_loss += loss.item()\n",
    "\n",
    "        flow_train_loss_avg = flow_train_loss / len(train_loader) \n",
    "        \n",
    "        # Validation set\n",
    "        flow.eval()\n",
    "        flow_val_loss = 0\n",
    "        for batch_idx, (val_params, val_data) in enumerate(test_loader):\n",
    "            #val_data = val_data.to(device)\n",
    "            val_params = val_params.to(device)\n",
    "            val_loss = -flow.log_prob(inputs=val_params).mean()#, context=val_data).mean()\n",
    "            #val_loss = -flow.log_prob(inputs=val_params, context=val_data).mean()\n",
    "            flow_val_loss += val_loss.item()\n",
    "\n",
    "        flow_val_loss_avg = flow_val_loss / len(test_loader)\n",
    "\n",
    "        flow_val_losses.append(flow_val_loss_avg)\n",
    "        flow_train_losses.append(flow_train_loss_avg)\n",
    "\n",
    "        time_it = (time.time()-time0)/60.\n",
    "        \n",
    "        if epoch % args.log_interval2 ==0: print('====> Epoch: {} Average loss: {:.4f} Validation loss: {:.4f} Time: {:.3f} min'.format(epoch, flow_train_loss_avg, flow_val_loss_avg, time_it))\n",
    "            \n",
    "    print('Training: {0:2.2f} min'.format( (time.time()-time0)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96368080-042f-48f0-8a6c-20430db4069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"MaskedAffineAutoregressiveTransform-4-32.pth\"\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': flow.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses_p': flow_train_losses,\n",
    "            'val_losses_p': flow_val_losses,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7640d3-daff-4374-9aa3-0ca641283505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert tensors to regular Python numbers (floats)\n",
    "df_losses_dict = {\n",
    "    'train': [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in flow_train_losses],\n",
    "    'val': [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in flow_val_losses]\n",
    "}\n",
    "\n",
    "# Now you can create the DataFrame\n",
    "df_losses = pd.DataFrame(df_losses_dict)\n",
    "# Continue with plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(data=df_losses)  # Plots the training and validation losses\n",
    "plt.ylabel('Losses')\n",
    "#plt.ylim(-1800, -500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7a480-0210-4d84-81f7-e7fe05e24952",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cycles_of_generated_samples = 100 # \n",
    "    how_many = 10 # width\n",
    "    range_range = 100 # length of the seeding vector\n",
    "    total_samples = range_range * how_many * cycles_of_generated_samples                                                                                                                                                          \n",
    "    time0 = time.time()\n",
    "    \n",
    "    print('Number of samples is ', range_range, 'x ',cycles_of_generated_samples, 'x ', how_many)\n",
    "\n",
    "    flow.eval()\n",
    "\n",
    "\n",
    "    conditional = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(range_range):\n",
    "            counter = 0\n",
    "            while counter < cycles_of_generated_samples:\n",
    "                if conditional == True:    \n",
    "                    conditional_tensor = y_s.to(device).float().unsqueeze(1)\n",
    "                    #sample = flow.sample(len(conditional_tensor), context=conditional_tensor)\n",
    "                    sample = flow.sample(how_many, context=conditional_tensor)\n",
    "                else:\n",
    "                    sample = flow.sample(how_many)\n",
    "                sample = np.array(sample.cpu())\n",
    "                if counter == 0 and i == 0:\n",
    "                    samples = sample\n",
    "                else:\n",
    "                    samples = np.vstack([samples, sample])\n",
    "                counter += 1\n",
    "            print('sample', i, ', {0:2.2f} min'.format( (time.time()-time0)/60.), samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee116222-28af-4292-a068-e807531ded4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #samples_transformed = np.array(samples)\n",
    "    samples = np.array(samples)#.cpu())\n",
    "    try:\n",
    "        samples_transformed = samples[:, :] # those generated from high QED\n",
    "#        samples_2 = samples[:, 1, :] # those generated of the same lengths\n",
    "#        samples_transformed = np.concatenate((samples_1, samples_2), axis=0) # 50 min for 200 samples\n",
    "        print(samples_transformed.shape, 'Conditional sampling: {0:2.2f} min'.format( (time.time()-time0)/60.))\n",
    "    except:\n",
    "        print(samples_transformed.shape, 'Sampling: {0:2.2f} min'.format( (time.time()-time0)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51af822-b8e3-4445-a0d6-50774ecb922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "samples_transformed_re = samples.reshape(-1, 292)#.head(32000)\n",
    "samples_transformed_re.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98089d24-dee6-4315-8efc-89d04dacbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_splits = samples.shape[0]/500\n",
    "split = np.array_split(samples, number_of_splits, axis=0)  # Split into X chunks along rows\n",
    "\n",
    "def process_decoding(part):\n",
    "    return np.float32(model.decode(torch.tensor(part).float().cuda()).cpu()) # decode in segments\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    processed_part = [process_decoding(iterator) for iterator in split] # apply function\n",
    "\n",
    "samples_decoded = np.vstack(processed_part) # combined together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13b9b7-8be2-408a-a473-106143658100",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smiles = []\n",
    "valid_smiles = []\n",
    "\n",
    "for id, molecule in enumerate(samples_decoded):\n",
    "    all_smiles.append(decode_smiles_from_indexes(molecule.reshape(1, 120, len(charset)).argmax(axis=2)[0], charset))\n",
    "\n",
    "for smi in all_smiles:\n",
    "    m = Chem.MolFromSmiles(smi,sanitize=False)\n",
    "    if m is None:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            Chem.SanitizeMol(m)\n",
    "            valid_smiles.append(smi)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print('%.2f' % (len(valid_smiles) / len(all_smiles)*100),  '% of generated samples are valid samples, where all smiles is ', len(all_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ab964-f16c-40b7-ad67-1af22b0a8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smiles[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc03203-5d99-4ee3-b1be-1deec92d5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "unique_smiles = OrderedDict((x, True) for x in valid_smiles).keys()\n",
    "print('%.2f' % (len(unique_smiles) / len(all_smiles)*100),  '% of generated samples are unique samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346c82b-9752-4468-9ffe-abfe9480d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating QED/SAS for ', len(unique_smiles), ' molecules out of all samples')\n",
    "df_generated = pd.DataFrame(unique_smiles, columns=[\"Original_SMILES\"]).drop_duplicates(subset=['Original_SMILES'])\n",
    "for index, row in df_generated.iterrows():\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES'])#+'OP(C)(=O)F')\n",
    "            qed = QED.default(mol)\n",
    "            try:\n",
    "                sas_score = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score = np.nan\n",
    "        except:\n",
    "            sas_score = np.nan\n",
    "            qed = np.nan\n",
    "        \n",
    "        df_generated.at[index, \"QED\"] = qed\n",
    "        df_generated.at[index, \"SA_score\"] = sas_score\n",
    "        df_generated.at[index, \"Origin\"] = str(cycle)+'_iter'\n",
    "\n",
    "new = df_generated.dropna(subset=['QED', 'SA_score']).sort_values(['QED'], ascending=False)\n",
    "print('%.2f' % (len(new) / len(all_smiles)*100),  '% of generated samples are unique samples with QED/SAS scores.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5e368-ffb1-4a92-84a1-b6d83811f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522681b-7a44-4638-881e-8e1c6ebd0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "named = 'MaskedAffineAutoregressive-4-32.csv'\n",
    "#new.to_csv(named, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa04c4-a49d-4503-8e9f-362aa27bbba3",
   "metadata": {},
   "source": [
    "# Are these datapoints new?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15c26b-b753-4130-bc2e-25aa83637d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_examples = pd.read_csv(seeding_dataset, index_col=0)\n",
    "all_examples['Origin'] = \"ChEMBL22-50k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd214f9-d614-4204-af8e-b40f3328b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = pd.merge(all_examples, new, on=['Original_SMILES'], how='inner')\n",
    "print('out of ', len(unique_smiles), ' unique samples', len(duplicates), 'were present in the original dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0964de-ccbe-4410-aa5f-cbb5951d7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('previous dataset had ', len(all_examples), ' data points')\n",
    "formed_new_dataset = pd.merge(all_examples, new, on=['Original_SMILES', 'QED', 'SA_score', 'Origin'], how='outer').drop_duplicates(subset=['Original_SMILES'], keep='first')\n",
    "formed_new_dataset.to_csv(str(cycle)+'_'+named, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782efae-60b6-4216-83c3-29f9cfd28cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "formed_new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b943d8-6a32-4c87-ab86-23850a016c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "formed_new_dataset.sort_values(['QED'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
