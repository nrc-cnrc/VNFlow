{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = '10 Iter'\n",
    "new.to_csv('data_cycles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYYoQ4WBpjUr"
   },
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o344x9rX-_gX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import properscoring as ps\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from nflows.nn.nets import ResidualNet\n",
    "from torch.nn import functional as F\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform, MaskedPiecewiseQuadraticAutoregressiveTransform, MaskedPiecewiseLinearAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.autoregressive import MaskedUMNNAutoregressiveTransform, MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.base import (\n",
    "    CompositeTransform,\n",
    "    InputOutsideDomain,\n",
    "    InverseTransform,\n",
    "    Transform)\n",
    "from nflows.transforms.splines.cubic import unconstrained_cubic_spline\n",
    "from nflows.transforms.autoregressive import AutoregressiveTransform\n",
    "from nflows.transforms import made as made_module\n",
    "from nflows.utils import torchutils\n",
    "from nflows.transforms.base import InputOutsideDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies as sf\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y09wxBAiMjKC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')  # suppress error messages\n",
    "\n",
    "import math # just for train/test split\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqxSwFo6FxnF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2abgDK-tp-jF"
   },
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0bsx1rCBq6B"
   },
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "  data = df['QED'].values.reshape((-1,1))\n",
    "    #  data = df[['QED', 'len']]#.values.reshape((-1,1)) # to get multiple column context\n",
    "  return data\n",
    "\n",
    "def preprocess_smiles_NaN(smiles):\n",
    "    try:\n",
    "        encoded = sf.encoder(smiles)\n",
    "    except:\n",
    "        print('Failed to convert'+str(smiles))\n",
    "        encoded = 'nop'    # substitute empty strings for [nop]\n",
    "    return encoded\n",
    "\n",
    "def preprocess_smiles(smiles):\n",
    "    encoded = sf.encoder(smiles)\n",
    "    return encoded\n",
    "\n",
    "def keys_int(symbol_to_int):\n",
    "  d={}\n",
    "  i=0\n",
    "  for key in symbol_to_int.keys():\n",
    "    d[i]=key\n",
    "    i+=1\n",
    "  return d\n",
    "\n",
    "def get_selfies(df):\n",
    "    global selfies_list, largest_selfie_len, int_mol, selfies_alphabet\n",
    "\n",
    "    sf.set_semantic_constraints()  # reset constraints\n",
    "    constraints = sf.get_semantic_constraints()\n",
    "    constraints['?'] = 3\n",
    "    sf.set_semantic_constraints(constraints)\n",
    "    \n",
    "    selfies_list = np.asanyarray(df.selfies)\n",
    "    selfies_alphabet = sf.get_alphabet_from_selfies(selfies_list)\n",
    "    # selfies_alphabet.add('.')  # test this\n",
    "    selfies_alphabet.add('[nop]')  # Add the \"no operation\" symbol as a padding character; selfies_alphabet.remove('[P]')\n",
    "    selfies_alphabet = list(sorted(selfies_alphabet))\n",
    "    # selfies_list = np.asanyarray(df.selfies) # added this line for a subset only if I want to load more selfie alphabet examples\n",
    "    largest_selfie_len = max(sf.len_selfies(s) for s in selfies_list)\n",
    "    symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "    int_mol=keys_int(symbol_to_int)\n",
    "    \n",
    "    onehots=sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int, largest_selfie_len)\n",
    "    input_tensor = tf.convert_to_tensor(onehots, dtype='float32')\n",
    "    noise_tensor = tf.random.uniform(shape=input_tensor.shape, minval=0, maxval=1, dtype='float32')\n",
    "    data = tf.add(input_tensor, noise_tensor) # dequantized data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../Dataset_after_cycle_4.csv',delimiter=',').dropna(subset=['QED']).drop_duplicates(subset=['Original_SMILES'])\n",
    "data = pd.read_csv('data_cycles.csv',delimiter=',').dropna(subset=['QED']).drop_duplicates(subset=['Original_SMILES'])\n",
    "#data = df[df[\"Origin\"] == 'Initial'] #df.copy()\n",
    "#data['len'] = data['selfies'].apply(lambda x: sf.len_selfies(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = data.sort_values(by=['QED']).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moYRZggM0Trg"
   },
   "outputs": [],
   "source": [
    "x = np.float32(get_selfies(data))\n",
    "y = np.float32(get_data(data))\n",
    "\n",
    "y = torch.tensor(y).float()\n",
    "x = torch.tensor(x).float()\n",
    "\n",
    "index = math.floor(x.shape[0]*1.0)\n",
    "\n",
    "x_train = x[:index]\n",
    "y_train = y[:index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = np.float32(get_data(data_s))\n",
    "# x_s = np.float32(get_data(data_s))\n",
    "\n",
    "y_s = torch.tensor(y_s).float()\n",
    "#x_s = torch.tensor(x_s).float()\n",
    "index = math.floor(y_s.shape[0]*1.0)\n",
    "\n",
    "# x_test = x_s[:index]\n",
    "y_test = y_s[:index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  x_train, y_train, y_test = x_train.cuda(), y_train.cuda(), y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo6vIglb_MXh"
   },
   "outputs": [],
   "source": [
    "num_layers = 12 # was 5\n",
    "hiddenfeatures = 16\n",
    "base_dist = StandardNormal(shape=[x_train.shape[1]])\n",
    "\n",
    "transforms = []\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    transforms.append(ReversePermutation(features=x_train.shape[1]))\n",
    "    transforms.append(MaskedAffineAutoregressiveTransform(features=x_train.shape[1],\n",
    "                                                          hidden_features=hiddenfeatures,\n",
    "                                                          context_features=1))\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist)\n",
    "if torch.cuda.is_available():\n",
    "  flow = flow.cuda()\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters(),lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3000,5000],\n",
    "                                           gamma=0.3) # was 300, 600, 0.3, with 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaFHFA4mAQfu",
    "outputId": "8b2a1ebf-d52f-4944-c9c2-0e6f31d6c3b1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_iter = 7500\n",
    "\n",
    "for i in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow.log_prob(inputs=x_train, context=y_train).mean()\n",
    "    if i%500 == 0:\n",
    "      print('iteration',i,':',loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFO1pA2LA703"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEHsnfg2cKuH",
    "outputId": "676a5387-8b45-45ff-caf8-81001b8f99bb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = []\n",
    "with torch.no_grad():\n",
    "  for i in range(y_true.shape[0]):\n",
    "      sample = flow.sample(1,context=y_test[i,:].reshape((-1,1))).cpu().numpy()\n",
    "      sample = sample.squeeze()\n",
    "      if i%10 == 0:\n",
    "          print('sample',i)\n",
    "      samples.append(list(sample))\n",
    "\n",
    "samples = np.array(samples)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = tf.math.floor(samples)  # quantize data\n",
    "mols = tf.clip_by_value(mols, 0, 1)  # Set negative values to 0 and values > 1 to 1\n",
    "mols_list = mols.numpy().tolist()\n",
    "\n",
    "# Add padding characters if needed\n",
    "for mol in mols_list:\n",
    "    for i in range(largest_selfie_len):\n",
    "        row = mol[len(selfies_alphabet) * i: len(selfies_alphabet) * (i + 1)]\n",
    "        if all(elem == 0 for elem in row):\n",
    "            mol[len(selfies_alphabet) * (i+1) - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols=sf.batch_flat_hot_to_selfies(mols_list, int_mol)\n",
    "\n",
    "valid_count = 0\n",
    "valid_selfies, invalid_selfies = [], []\n",
    "for idx, selfies in enumerate(mols):\n",
    "  try:\n",
    "    if Chem.MolFromSmiles(sf.decoder(mols[idx]), sanitize=True) is not None:\n",
    "        valid_count += 1\n",
    "        valid_selfies.append(selfies)\n",
    "    else:\n",
    "      invalid_selfies.append(selfies)\n",
    "  except Exception:\n",
    "    pass\n",
    "print('%.2f' % (valid_count / len(mols)),  '% of generated samples are valid molecules.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mols = [Chem.MolFromSmiles(sf.decoder(vs)+'OP(C)(=O)F') for vs in valid_selfies]\n",
    "\n",
    "gen_mols = [i for i in gen_mols if i]\n",
    "\n",
    "smiles_generated = [sf.decoder(vs) for vs in valid_selfies] #+'OP(C)(=O)F' for vs in valid_selfies]\n",
    "\n",
    "from collections import OrderedDict\n",
    "OrderedDict((x, True) for x in smiles_generated).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_range_min = data.idxmax()[0]+1\n",
    "df_generated = pd.DataFrame(smiles_generated, columns=[\"Original_SMILES\"])#.drop_duplicates()\n",
    "df_generated['Context'] = y_true.reshape((-1,1))\n",
    "#df_generated['len'] = df_generated['Original_SMILES'].apply(lambda x: len(x))\n",
    "df_generated['selfies'] = df_generated['Original_SMILES'].apply(preprocess_smiles_NaN)\n",
    "df_generated['RowID'] = pd.Series(samples.shape[0])\n",
    "df_generated['Origin'] = iteration\n",
    "df_generated.index += my_range_min\n",
    "new = pd.concat([data, df_generated])#.drop_duplicates(subset=['Original_SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new.iterrows():\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(row['Original_SMILES']+'OP(C)(=O)F')\n",
    "        qed = QED.default(mol)\n",
    "        try:\n",
    "            sas_score = sascorer.calculateScore(mol)\n",
    "        except:\n",
    "            sas_score = np.nan \n",
    "    except:\n",
    "        sas_score = np.nan\n",
    "        qed = np.nan\n",
    "    \n",
    "    new.at[index, \"QED\"] = qed\n",
    "    new.at[index, \"SA_score\"] = sas_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['SELFIES_Length'] = new['selfies'].apply(lambda x: sf.len_selfies(x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#ax.hist(new_vis['SELFIES_Length'], 15, density=True, histtype='step')#, stepped=True)\n",
    "ax = sns.histplot(data=new, x=\"SELFIES_Length\", hue=\"Origin\", kde=True, element=\"step\", bins = 15, multiple=\"stack\")\n",
    "ax.set_xlabel('Selfie String Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#ax.hist(new_vis['SELFIES_Length'], 15, density=True, histtype='step')#, stepped=True)\n",
    "ax = sns.histplot(data=new, x=\"QED\", hue=\"Origin\", kde=True, element=\"step\", bins = 20, multiple=\"stack\")\n",
    "ax.set_xlabel('QED')\n",
    "plt.axvline(x=0.712277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = new[new[\"Origin\"] == iteration]\n",
    "#y_new['Context [QED]']= y_test.cpu().numpy().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=new, x=\"QED\", y=\"Context\", hue=\"Origin\")\n",
    "ax.set_xlabel('Calculated QED for the sample molecule')\n",
    "#x.set_ylim(bottom=0.02, top=0.80)\n",
    "#ax.set_xlim(left=0.02, right=0.80)\n",
    "ax.legend(bbox_to_anchor=(1.0, 1.00))\n",
    "plt.axvline(x=0.712277)\n",
    "plt.axhline(y=0.712277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "\n",
    "analysis_input=sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int, largest_selfie_len)\n",
    "analysis_input_results = np.zeros((len(analysis_input[0]),), dtype=int).tolist()\n",
    "\n",
    "for i in analysis_input: #range(0, 2):#len(analysis_input)):\n",
    "    #print(i)\n",
    "    summa = []\n",
    "    for k in range(0, len(analysis_input[0])):\n",
    "        #print(analysis_input_results[k] , 'and',  i[k])\n",
    "        summa.append(analysis_input_results[k] + i[k])\n",
    "    analysis_input_results = summa\n",
    "\n",
    "def split_list(lst, chunk_size):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "input_analysis = split_list(analysis_input_results, len(selfies_alphabet))\n",
    "# symbol_to_int # = x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "\n",
    "analysis_output=sf.batch_selfies_to_flat_hot(mols, symbol_to_int, largest_selfie_len)\n",
    "analysis_output_results = np.zeros((len(analysis_output[0]),), dtype=int).tolist()\n",
    "\n",
    "for i in analysis_output: #range(0, 2):#len(analysis_input)):\n",
    "    #print(i)\n",
    "    summa = []\n",
    "    for k in range(0, len(analysis_output[0])):\n",
    "        summa.append(analysis_output_results[k] + i[k])\n",
    "    analysis_output_results = summa\n",
    "\n",
    "output_analysis = split_list(analysis_output_results, len(selfies_alphabet))\n",
    "\n",
    "histogram_inp = dict(list(enumerate(input_analysis)))\n",
    "histogram_out = dict(list(enumerate(output_analysis)))\n",
    "\n",
    "histogram_inp['Selfie'] = selfies_alphabet\n",
    "histogram_out['Selfie'] = selfies_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(24, 24))\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.05, \n",
    "                    right=0.99, \n",
    "                    top=0.97,\n",
    "                    wspace=0.3, \n",
    "                    hspace=0.6\n",
    "                   )\n",
    "\n",
    "count=0\n",
    "position=[axes[0, 0], axes[0, 1], axes[0, 2], axes[0, 3],\n",
    "          axes[1, 0], axes[1, 1], axes[1, 2], axes[1, 3],\n",
    "          axes[2, 0], axes[2, 1], axes[2, 2], axes[2, 3],\n",
    "          axes[3, 0], axes[3, 1], axes[3, 2], axes[3, 3]\n",
    "         ]\n",
    "\n",
    "names =     [\"0th\", \"1st\", \"2nd\", \"3rd\",\n",
    "             \"4th\", \"5th\", \"6th\", \"7th\",\n",
    "             \"8th\", \"9th\", \"10th\", \"11th\",\n",
    "             \"12th\", \"13th\", \"14th\", \"15th\"]\n",
    "\n",
    "for ax, title in zip(axes.flat, names):\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "for i in range(0, 16):\n",
    "    data_vis = pd.DataFrame()\n",
    "    \n",
    "    data_vis['Selfie'] = selfies_alphabet\n",
    "    data_vis['Input'] = histogram_inp[i]\n",
    "    data_vis['Output'] = histogram_out[i]\n",
    "    \n",
    "    data_vis.plot(ax=position[count], kind='bar', x='Selfie', y=['Input', 'Output'], rot = 90)\n",
    "\n",
    "    position[count].set_yscale('log')\n",
    "#    position[count].set_ylim(top=200)\n",
    "    \n",
    "    # deleting the duplicate x and y labels\n",
    "    axes[0, 0].set_xlabel('', fontsize=20)\n",
    "    axes[0, 1].set_xlabel('', fontsize=20)\n",
    "    axes[0, 2].set_xlabel('', fontsize=20)\n",
    "    axes[0, 3].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 0].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 1].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 2].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 3].set_xlabel('', fontsize=20)\n",
    "    axes[2, 0].set_xlabel('', fontsize=20)\n",
    "    axes[2, 1].set_xlabel('', fontsize=20)\n",
    "    axes[2, 2].set_xlabel('', fontsize=20)\n",
    "    axes[2, 3].set_xlabel('', fontsize=20)\n",
    "    \n",
    "    count=count+1\n",
    "\n",
    "fig.savefig('Distributions-in-out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_up = pd.read_csv('../Dataset_after_cycle_4.csv',delimiter=',')#.dropna(subset=['QED']).drop_duplicates(subset=['Original_SMILES'])\n",
    "\n",
    "for index, row in to_clean_up.iterrows():\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(row['Original_SMILES']+'OP(C)(=O)F')\n",
    "        qed = QED.default(mol)\n",
    "        try:\n",
    "            sas_score = sascorer.calculateScore(mol)\n",
    "        except:\n",
    "            sas_score = np.nan \n",
    "    except:\n",
    "        sas_score = np.nan\n",
    "        qed = np.nan\n",
    "    \n",
    "    to_clean_up.at[index, \"QED\"] = qed\n",
    "    to_clean_up.at[index, \"SA_score\"] = sas_score\n",
    "\n",
    "to_clean_up.to_csv('../Dataset_after_cycle_4-corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#ax.hist(new_vis['SELFIES_Length'], 15, density=True, histtype='step')#, stepped=True)\n",
    "ax = sns.histplot(data=to_clean_up, x=\"QED\", hue=\"Origin\", kde=True, element=\"step\", bins = 20, multiple=\"stack\")\n",
    "ax.set_xlabel('QED')\n",
    "plt.axvline(x=0.712277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_iter = 5000\n",
    "num_samples = 50\n",
    "\n",
    "for iteration in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:\n",
    "    print('Iteration '+iteration)\n",
    "    torch.cuda.empty_cache()\n",
    "    data_s = new.sort_values(by=['QED']).tail(num_samples)\n",
    "\n",
    "    # x and y to train:\n",
    "    x = np.float32(get_selfies(new))\n",
    "    y = np.float32(get_data(new))\n",
    "    x = torch.tensor(x).float()\n",
    "    y = torch.tensor(y).float()\n",
    "    index = math.floor(x.shape[0]*1.0)\n",
    "    x_train = x[:index]\n",
    "    y_train = y[:index,:]\n",
    "\n",
    "    y_s = np.float32(get_data(data_s))\n",
    "    y_s = torch.tensor(y_s).float()\n",
    "    index = math.floor(y_s.shape[0]*1.0)\n",
    "    y_test = y_s[:index,:]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x_train, y_train, y_test = x_train.cuda(), y_train.cuda(), y_test.cuda()\n",
    "\n",
    "    #base_dist = StandardNormal(shape=[x_train.shape[1]])\n",
    "    #transforms = []\n",
    "\n",
    "    #for _ in range(num_layers):\n",
    "    #    transforms.append(ReversePermutation(features=x_train.shape[1]))\n",
    "    #    transforms.append(MaskedAffineAutoregressiveTransform(features=x_train.shape[1],\n",
    "    #                                                          hidden_features=hiddenfeatures,\n",
    "    #                                                          context_features=1))\n",
    "    #    \n",
    "    #transform = CompositeTransform(transforms)\n",
    "    #flow = Flow(transform, base_dist)\n",
    "    #if torch.cuda.is_available():\n",
    "    #    flow = flow.cuda()\n",
    "\n",
    "    #optimizer = optim.Adam(flow.parameters(),lr=1e-4)\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1500,3000],\n",
    "    #                                           gamma=0.3)\n",
    "\n",
    "    print('x train is ',x_train.shape,', y train is ', y_train.shape, ', sampling is', y_test.shape)\n",
    "    for i in range(num_iter):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -flow.log_prob(inputs=x_train, context=y_train).mean()\n",
    "        print(-flow.log_prob(inputs=x_train, context=y_train))\n",
    "        if i%500 == 0:\n",
    "            print('iteration',i,':',loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    samples = []\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    with torch.no_grad():\n",
    "      for i in range(y_true.shape[0]):\n",
    "          sample = flow.sample(1,context=y_test[i,:].reshape((-1,1))).cpu().numpy()\n",
    "          sample = sample.squeeze()\n",
    "          if i%20 == 0:\n",
    "              print('sample',i)\n",
    "          samples.append(list(sample))\n",
    "\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    mols = tf.math.floor(samples)  # quantize data\n",
    "    mols = tf.clip_by_value(mols, 0, 1)  # Set negative values to 0 and values > 1 to 1\n",
    "    mols_list = mols.numpy().tolist()\n",
    "\n",
    "    for mol in mols_list:\n",
    "      for i in range(largest_selfie_len):\n",
    "        row = mol[len(selfies_alphabet) * i: len(selfies_alphabet) * (i + 1)]\n",
    "        if all(elem == 0 for elem in row):\n",
    "            mol[len(selfies_alphabet) * (i+1) - 1] = 1\n",
    "\n",
    "    mols=sf.batch_flat_hot_to_selfies(mols_list, int_mol)\n",
    "\n",
    "    valid_count = 0\n",
    "    valid_selfies, invalid_selfies = [], []\n",
    "    for idx, selfies in enumerate(mols):\n",
    "        try:\n",
    "            if Chem.MolFromSmiles(sf.decoder(mols[idx]), sanitize=True) is not None:\n",
    "                valid_count += 1\n",
    "                valid_selfies.append(selfies)\n",
    "            else:\n",
    "                invalid_selfies.append(selfies)\n",
    "        except Exception:\n",
    "          pass\n",
    "          \n",
    "    print('%.2f' % (valid_count / len(mols)),  '% of generated samples are valid molecules.')\n",
    "\n",
    "    gen_mols = [Chem.MolFromSmiles(sf.decoder(vs)+'OP(C)(=O)F') for vs in valid_selfies]\n",
    "    gen_mols = [i for i in gen_mols if i]\n",
    "    smiles_generated = [sf.decoder(vs) for vs in valid_selfies] #+'OP(C)(=O)F' for vs in valid_selfies]\n",
    "    print(OrderedDict((x, True) for x in smiles_generated).keys())\n",
    "\n",
    "    df_generated = pd.DataFrame(smiles_generated, columns=[\"Original_SMILES\"])#.drop_duplicates()\n",
    "    df_generated['len'] = df_generated['Original_SMILES'].apply(lambda x: len(x))\n",
    "    df_generated['Context'] = y_true.reshape((-1,1))\n",
    "    df_generated['selfies'] = df_generated['Original_SMILES'].apply(preprocess_smiles_NaN)\n",
    "    df_generated['RowID'] = pd.Series(samples.shape[0])\n",
    "    df_generated['Origin'] = str(iteration)+' Iter'\n",
    "    df_generated.index += data.idxmax()+1 #my_range_min\n",
    "    new = pd.concat([data, df_generated]).drop_duplicates(subset=['Original_SMILES'])\n",
    "\n",
    "    for index, row in new.iterrows():\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES']+'OP(C)(=O)F')\n",
    "            qed = QED.default(mol)\n",
    "            try:\n",
    "                sas_score = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score = np.nan \n",
    "        except:\n",
    "            sas_score = np.nan\n",
    "            qed = np.nan\n",
    "    \n",
    "    new.at[index, \"QED\"] = qed\n",
    "    new.at[index, \"SA_score\"] = sas_score"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "wind power - normalizing flow",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
