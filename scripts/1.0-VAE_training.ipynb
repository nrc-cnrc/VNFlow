{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3397d-3d15-4747-8b8a-15ddc0b9d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_start = 1\n",
    "seeding_dataset = 'data/ChemBL-35-cleaned.csv' # initialized with 50k-ChemBL.csv\n",
    "h5_input_data = 'data/'+str(cycle_start)+'-inp.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c826636-ecd3-4d4c-978c-de7585603787",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data File Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7c367-4a5b-4b96-92a2-e23b7cd94854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from molecules.utils import one_hot_array, one_hot_index\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from group_selfies import (\n",
    "    fragment_mols, \n",
    "    Group, \n",
    "    MolecularGraph, \n",
    "    GroupGrammar, \n",
    "    group_encoder\n",
    ")\n",
    "\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "import IPython.display # from ... import display\n",
    "from test_utils import *\n",
    "from rdkit import RDLogger\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "import os\n",
    "import sys\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit.Chem import QED\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import gzip\n",
    "import pandas\n",
    "import h5py\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn import model_selection\n",
    "     \n",
    "\n",
    "def one_hot_array(i, n):\n",
    "    return map(int, [ix == i for ix in xrange(n)])\n",
    "\n",
    "def one_hot_index(vec, charset):\n",
    "    return map(charset.index, vec)\n",
    "\n",
    "def from_one_hot_array(vec):\n",
    "    oh = np.where(vec == 1)\n",
    "    if oh[0].shape == (0, ):\n",
    "        return None\n",
    "    return int(oh[0][0])\n",
    "\n",
    "def decode_smiles_from_indexes(vec, charset):\n",
    "    # Ensure that each element in 'vec' is a string (not numpy.bytes_)\n",
    "    return \"\".join(map(lambda x: str(charset[x], 'utf-8') if isinstance(charset[x], bytes) else charset[x], vec)).strip()\n",
    "\n",
    "def load_dataset(filename, split = True):\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "    if split:\n",
    "        data_train = h5f['data_train'][:]\n",
    "    else:\n",
    "        data_train = None\n",
    "    data_test = h5f['data_test'][:]\n",
    "    charset =  h5f['charset'][:]\n",
    "    h5f.close()\n",
    "    if split:\n",
    "        return (data_train, data_test, charset)\n",
    "    else:\n",
    "        return (data_test, charset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d65562e-6c25-4e6f-9976-5f0079839bc3",
   "metadata": {},
   "source": [
    "#old code for reading old datasets\n",
    "data_train, data_test, charset = load_dataset('processed.h5')\n",
    "data_train = torch.utils.data.TensorDataset(torch.from_numpy(data_train))\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=1, shuffle=True)\n",
    "\n",
    "data_test = torch.utils.data.TensorDataset(torch.from_numpy(data_test))\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48fa1fa9-c764-42f1-be3d-b39f21c2042e",
   "metadata": {},
   "source": [
    "    #old code for reading old datasets\n",
    "    dataset = []\n",
    "\n",
    "    for batch_idx, val_data in enumerate(train_loader):\n",
    "    #if batch_idx == 2:\n",
    "        #print(val_data)\n",
    "        point = decode_smiles_from_indexes(map(from_one_hot_array, val_data[0][0]), charset)\n",
    "        dataset.append(point)\n",
    "\n",
    "    for batch_idx, val_data in enumerate(test_loader):\n",
    "    #if batch_idx == 2:\n",
    "        #print(val_data)\n",
    "        point = decode_smiles_from_indexes(map(from_one_hot_array, val_data[0][0]), charset)\n",
    "        dataset.append(point)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a4a4466-8d35-493c-954e-385192fe77d0",
   "metadata": {},
   "source": [
    "merged = pd.read_csv(seeding_dataset)\n",
    "\n",
    "for index, row in merged.iterrows():\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES'])#+'OP(C)(=O)F')\n",
    "            qed = QED.default(mol)\n",
    "            try:\n",
    "                sas_score = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score = np.nan\n",
    "        except:\n",
    "            sas_score = np.nan\n",
    "            qed = np.nan\n",
    "\n",
    "        merged.at[index, \"QED\"] = qed\n",
    "        merged.at[index, \"SA_score\"] = sas_score\n",
    "\n",
    "merged.to_csv('data/init_'+seeding_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488d96a-7d3e-44d5-8154-d401c3eceedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_ROWS = 100000000 #all\n",
    "SMILES_COL_NAME = 'Original_SMILES' # smiles\n",
    "PROPERTY_COL_NAME = 'QED'\n",
    "PROPERTY_COL_NAME2 = 'SA_score'\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "class dotdict(dict): \n",
    "  __getattr__ = dict.get\n",
    "  __setattr__ = dict.__setitem__\n",
    "  __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict()\n",
    "args.infile = seeding_dataset\n",
    "args.outfile = h5_input_data\n",
    "args.length = MAX_NUM_ROWS\n",
    "args.smiles_column = SMILES_COL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06650f2d-6556-4af4-aeb6-52178b5b45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_iterator(dataset, chunk_size=CHUNK_SIZE):\n",
    "    # Split the indices into chunks\n",
    "    chunk_indices = np.array_split(np.arange(len(dataset)), len(dataset) // chunk_size)\n",
    "    for chunk_ixs in chunk_indices:\n",
    "        chunk = dataset[chunk_ixs]\n",
    "        yield (chunk_ixs, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5927ef6-7ad2-4d28-a7af-d4d88c4c346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_dataset(h5file, dataset_name, dataset, dataset_shape,\n",
    "                         chunk_size=CHUNK_SIZE, apply_fn=None):\n",
    "    # Create the HDF5 dataset with the specified shape and chunk size\n",
    "    new_data = h5file.create_dataset(dataset_name, dataset_shape,\n",
    "                                     chunks=tuple([chunk_size] + list(dataset_shape[1:])))\n",
    "    \n",
    "    # Iterate through chunks\n",
    "    for chunk_ixs, chunk in chunk_iterator(dataset):\n",
    "        if apply_fn:\n",
    "            encoded_data = np.array([list(apply_fn(i)) for i in chunk], dtype=np.float32)\n",
    "        else:\n",
    "            encoded_data = np.array(chunk, dtype=np.float32)\n",
    "\n",
    "        # Assign the encoded data back into the HDF5 dataset\n",
    "        new_data[chunk_ixs.tolist(), ...] = encoded_data\n",
    "\n",
    "\n",
    "def one_hot_encoded_fn(row):\n",
    "    # This function should return a list, not a map\n",
    "    result = [one_hot_array(x, len(charset)) for x in one_hot_index(row, charset)]\n",
    "    return result\n",
    "\n",
    "def one_hot_array(i, n):\n",
    "    #print(f\"One hot array for index {i} of size {n}\")\n",
    "    return [int(ix == i) for ix in range(n)]\n",
    "\n",
    "def one_hot_index(vec, charset):\n",
    "    return [charset.index(x) for x in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed320d87-5474-4f6b-936d-3a4de0bb382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(args.infile)\n",
    "keys = data[args.smiles_column].map(len) < 121 # # Filter rows based on SMILES length\n",
    "\n",
    "if args.length <= len(keys):\n",
    "    data = data[keys].sample(n=args.length)\n",
    "else:\n",
    "    data = data[keys]\n",
    "\n",
    "# Ensure that all SMILES strings are padded to 120 characters\n",
    "structures = data[args.smiles_column].map(lambda x: list(x.ljust(120)))\n",
    "\n",
    "if args.property_column:\n",
    "    properties = data[args.property_column][keys]\n",
    "\n",
    "if args.property_column2:\n",
    "    properties2 = data[args.property_column2][keys]\n",
    "\n",
    "del data  # Clean up to save memory\n",
    "\n",
    "train_idx, test_idx = map(np.array, train_test_split(structures.index, test_size=0.05, random_state=42))\n",
    "\n",
    "# Create the charset from the unique characters in the SMILES strings\n",
    "# charset = list(reduce(lambda x, y: set(y) | x, structures, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda1a94-b95e-4081-9814-04edb014f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = ['O', 'a', '[', 'e', 'K', 't', '4', 'o', '1', 'P', ']', 'p', 'l', 'X', '8', '3', 'Z', '-', 'S', 'L', '=', 'F', 'M', '.', 'C', ' ', 'r', 'T', 'N', '2', '0', 'R', '5', 'i', '/', 'b', 's', '+', '9', 'H', 'c', '@', '(', 'I', 'g', 'A', 'B', '7', '6', '#', '%', '\\\\', ')', 'n']\n",
    "print(len(charset))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a227c74-d6b5-4a2d-a520-4ff66447bb00",
   "metadata": {},
   "source": [
    "with h5py.File(args.outfile, 'w') as h5f:\n",
    "    h5f.create_dataset('charset', data=charset)\n",
    "    print('initiated')\n",
    "    # Create datasets for the training and testing data\n",
    "    create_chunk_dataset(h5f, 'data_train', train_idx,\n",
    "                         (len(train_idx), 120, len(charset)),\n",
    "                         apply_fn=lambda ch: one_hot_encoded_fn(structures[ch]))\n",
    "    print('train done')\n",
    "    create_chunk_dataset(h5f, 'data_test', test_idx,\n",
    "                         (len(test_idx), 120, len(charset)),\n",
    "                         apply_fn=lambda ch: one_hot_encoded_fn(structures[ch]))\n",
    "    print('file done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eee781-7751-42a3-ae57-f298b95d9a05",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5510adc-294e-433e-9c29-e6697eaecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import gzip\n",
    "import h5py\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79fa13-e70f-4499-bc6c-13db04b1546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molecules.model import vae_loss, dotdict, one_hot_index, decode_smiles_from_indexes, load_dataset\n",
    "from molecules.model import load_dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36956c-b32f-4316-a800-c73dea92b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def from_one_hot_array(vec):\n",
    "    oh = np.where(vec == 1)\n",
    "    if oh[0].shape == (0, ):\n",
    "        return None\n",
    "    return int(oh[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39908c-8437-44b4-926c-4fdafb116b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(120, 9, kernel_size=9)\n",
    "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "        self.linear_0 = nn.Linear(280, 435) # changed from 70 to 280 to reflect the change of charset size\n",
    "        self.linear_1 = nn.Linear(435, 292)\n",
    "        self.linear_2 = nn.Linear(435, 292)\n",
    "        \n",
    "        self.linear_3 = nn.Linear(292, 292)\n",
    "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(501, 54) # changed this output from 33 to 54 to reflect the larger charset size\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "        return self.linear_1(x), self.linear_2(x)\n",
    "\n",
    "    def sampling(self, z_mean, z_logvar):\n",
    "        epsilon = 1e-2 * torch.randn_like(z_logvar)\n",
    "        return torch.exp(0.5 * z_logvar) * epsilon + z_mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)\n",
    "        output, hn = self.gru(z)\n",
    "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.sampling(z_mean, z_logvar)\n",
    "        return self.decode(z), z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb006ae-4fcf-49c7-aaf7-87f324ec342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867afba6-ab8d-4894-90a6-7e9a70eb2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MolecularVAE().to(device)\n",
    "\n",
    "train_losses_p = []\n",
    "val_losses_p = []\n",
    "optimizer = optim.Adam(model.parameters())#, lr=1e-4) # First 60 with 1e-3, next 60 with 5.e-4, lastly with 1.e-4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17c51822-f6c8-4af4-8acc-1a856580b210",
   "metadata": {},
   "source": [
    "saved_charset = ['O', 'a', '[', 'e', 'K', 't', '4', 'o', '1', 'P', ']', 'p', 'l', 'X', '8', '3', 'Z', '-', 'S', 'L', '=', 'F', 'M', '.', 'C', ' ', 'r', 'T', 'N', '2', '0', 'R', '5', 'i', '/', 'b', 's', '+', '9', 'H', 'c', '@', '(', 'I', 'g', 'A', 'B', '7', '6', '#', '%', '\\\\', ')', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d4f78-4dd0-4dc0-813d-8b2b8f6eec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_losses_graph(train_losses_p, val_losses_p, ymax=100, ymin=0, printed=20):\n",
    "    df_losses_dict = {\n",
    "    'train': [float(loss) for loss in train_losses_p],\n",
    "    'val': [float(loss) for loss in val_losses_p],\n",
    "    }\n",
    "    df_losses = pd.DataFrame(df_losses_dict).reset_index(drop=True)#, inplace=True)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.lineplot(df_losses)\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim(ymin, ymax)\n",
    "    sns.set_style(\"ticks\")\n",
    "    #plt.yscale('log')\n",
    "    losses_graph = pd.DataFrame.from_dict(df_losses_dict)\n",
    "    losses_graph.index = losses_graph.index + 1\n",
    "    print(losses_graph.tail(printed))\n",
    "    print(losses_graph.nsmallest(3,['val']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9aba72c-cda2-4dba-a1a8-26060da80239",
   "metadata": {},
   "source": [
    "PATH = \"checkpoint_239.pth\" # 222\n",
    "\n",
    "checkpoint = torch.load(PATH, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_losses_p = checkpoint['train_losses_p']\n",
    "val_losses_p = checkpoint['val_losses_p']\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53962a45-d11c-45f9-a083-1371838d164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_losses_graph(train_losses_p, val_losses_p, printed=20, ymax=15, ymin=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e2416-d216-4f8e-957a-8057d1d49fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 100 # was 500\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # First 60 with 1e-3, next 60 with 5.e-4, lastly with 1.e-4 | 35/17 train/val when 200 batch size and 10-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03896270-f7d9-45ae-94a2-7d8b33c31350",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "args.epochs = 300 # was 30\n",
    "args.report_epochs = 1\n",
    "args.report_epochs2 = args.epochs/5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def train(epoch):\n",
    "    time1 = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    counter = 0\n",
    "    printed_batch = np.random.randint(0, 45)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_batches, data_test, charset = load_dataset_chunked(h5_input_data, split=True, batch_size=50000)\n",
    "    data_test = torch.utils.data.TensorDataset(torch.from_numpy(data_test))\n",
    "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    for batch in train_batches:\n",
    "        if counter == 20:\n",
    "            timeit = (time.time()-time1)/60.\n",
    "            timeit_all = 2 * timeit\n",
    "            print(f\"Processing batch {counter:.0f} shaped: {batch.shape}, done in {timeit:.2f} min, so 44 batches around {timeit_all:.0f} min\")\n",
    "        \n",
    "        batch_tensor = torch.from_numpy(batch)\n",
    "        current_train_batch = torch.utils.data.TensorDataset(batch_tensor)\n",
    "        train_loader = torch.utils.data.DataLoader(current_train_batch, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data[0]#.reshape(data[0].shape[0], 30, 107)\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, mean, logvar = model(data)\n",
    "            loss = vae_loss(output, data, mean, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if counter == printed_batch:\n",
    "            inp = data[0].cpu().numpy()\n",
    "            outp = output.cpu().detach().numpy()\n",
    "            sampled = outp[0].reshape(1, 120, len(charset)).argmax(axis=2)[0]\n",
    "            print(\"Input/Label vs Reconstructed from training set, \", str(counter), \" :\")\n",
    "            print(decode_smiles_from_indexes(map(from_one_hot_array, inp), charset))\n",
    "            print(decode_smiles_from_indexes(sampled, charset))\n",
    "        counter = counter + 1\n",
    "        \n",
    "    train_loss_p = train_loss / 2258093 # len(train_loader.dataset) # .cpu().detach().numpy()\n",
    "    train_losses_p.append(train_loss_p)\n",
    "    print(train_loss_p)\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_idx, val_data in enumerate(test_loader):\n",
    "            val_data = val_data[0].to(device)\n",
    "            output_val, mean, log_var = model(val_data)\n",
    "            val_loss += vae_loss(output_val, val_data, mean, log_var).item()\n",
    "\n",
    "        val_loss_p = val_loss / len(test_loader.dataset)\n",
    "        val_losses_p.append(val_loss_p)\n",
    "        \n",
    "        inp = val_data[0].cpu().numpy()\n",
    "        outp = output_val.cpu().detach().numpy()\n",
    "        sampled = outp[0].reshape(1, 120, len(charset)).argmax(axis=2)[0]\n",
    "        print(\"Input/Label vs Reconstructed from validation set:\")\n",
    "        print(decode_smiles_from_indexes(map(from_one_hot_array, inp), charset))\n",
    "        print(decode_smiles_from_indexes(sampled, charset))\n",
    "\n",
    "    timeit = (time.time()-time0)/60.\n",
    "    print(f'Epoch [{epoch}], Loss: {train_loss_p:.4f}, Validation Loss: {val_loss_p:.4f}, Time: {timeit:.2f} min')\n",
    "\n",
    "    #if epoch % args.report_epochs2 == 0:\n",
    "    PATH = \"checkpoint_\"+str(epoch)+\".pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses_p': train_losses_p,\n",
    "        'val_losses_p': val_losses_p,}, PATH)\n",
    "    \n",
    "    return train_loss_p    \n",
    "\n",
    "\n",
    "# keep here\n",
    "last_epoch = len(train_losses_p)\n",
    "for epoch in range(last_epoch +1, args.epochs + 1):\n",
    "    print('Initiating Epoch ', epoch)\n",
    "    train_loss = train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca10e0-2d8a-4ecb-8244-779fd8010e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trained on ', len(train_losses_p), len(val_losses_p), ' epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee9b1e-c1b5-4903-8bb0-dbcc972affca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_losses_dict = {\n",
    "    'train': [float(loss) for loss in train_losses_p],\n",
    "    'val': [float(loss) for loss in val_losses_p],\n",
    "}\n",
    "\n",
    "#df_losses_dict = {'train': train_losses_p, 'val': val_losses_p} \n",
    "df_losses = pd.DataFrame(df_losses_dict).reset_index(drop=True)#, inplace=True)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(df_losses)\n",
    "plt.ylabel('Losses')\n",
    "#plt.ylim(25, 50)\n",
    "sns.set_style(\"ticks\")\n",
    "#sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "#plt.xlim(0, 490)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738acf6-eaed-44b3-9d62-6cb29b26e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "PATH = \"VAE_new_\"+str(cycle_start)+\"-cycle.pth\"\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses_p': train_losses_p,\n",
    "            'val_losses_p': val_losses_p,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ac644-1773-4d7b-8aaf-4a65c17a2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img in train_loader:\n",
    "        mean, _ = model.encode(img[0].to(device))\n",
    "        all_means.append(mean.cpu())\n",
    "        for imagine in img[0]:\n",
    "            all_labels.append(decode_smiles_from_indexes(map(from_one_hot_array, imagine), charset))\n",
    "\n",
    "latent_data = torch.cat(all_means, dim=0)\n",
    "\n",
    "# Plotting with color coding for labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_data[:, 0], latent_data[:, 1], alpha=0.5, s=1)\n",
    "#plt.title(\"Latent Space Representation with Labels\")\n",
    "plt.xlabel(\"Latent Dimension 1\")\n",
    "plt.ylabel(\"Latent Dimension 2\")\n",
    "#plt.colorbar(label='Length')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42516707-f9ac-4725-9828-87085b210874",
   "metadata": {},
   "source": [
    "training_set_org = pd.DataFrame(all_labels, columns=[\"Original_SMILES\"])\n",
    "for index, row in training_set_org.iterrows():\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES'])#+'OP(C)(=O)F')\n",
    "            qed = QED.default(mol)\n",
    "            try:\n",
    "                sas_score = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score = np.nan\n",
    "        except:\n",
    "            sas_score = np.nan\n",
    "            qed = np.nan\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES']+'OP(C)(=O)F')\n",
    "            qed_o = QED.default(mol)\n",
    "            try:\n",
    "                sas_score_o = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score_o = np.nan\n",
    "        except:\n",
    "            sas_score_o = np.nan\n",
    "            qed_o = np.nan\n",
    "\n",
    "        \n",
    "        training_set_org.at[index, \"QED\"] = qed\n",
    "        training_set_org.at[index, \"SA_score\"] = sas_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6671ac3a-084c-4369-9c6a-d8444f74ee80",
   "metadata": {},
   "source": [
    "training_set_org.to_csv('data/1-training_set_org.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a7fd901-dd18-4b70-8c6f-9722cd47e08d",
   "metadata": {},
   "source": [
    "latent_data_numpy = latent_data.cpu().detach().numpy()\n",
    "\n",
    "# Convert the numpy array to a DataFrame\n",
    "latent_data_df = pd.DataFrame(latent_data_numpy)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "latent_data_df.to_csv('data/1-training_set_latent_space.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c3192-fb10-45ac-95d7-f3a7d4cfadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
