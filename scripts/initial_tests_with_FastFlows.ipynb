{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece2770-2064-45f5-b835-3bd69ac11eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = '10 Iter'\n",
    "#saving.to_csv('data_cycles-40.csv') # for 6th and further increased hidden from 16 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586e13f-adcc-4442-b3be-fb923e4da1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import deepchem as dc\n",
    "from deepchem.models.normalizing_flows import NormalizingFlow, NormalizingFlowModel\n",
    "from deepchem.models.optimizers import Adam\n",
    "from deepchem.data import NumpyDataset\n",
    "from deepchem.splits import RandomSplitter\n",
    "from deepchem.molnet import load_tox21\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import selfies as sf\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "# \"set the environment variable `TF_USE_LEGACY_KERAS=True` to \"\n",
    "# \"configure TensorFlow to use `tf_keras` when accessing `tf.keras`.\" \n",
    "#ImportError: `keras.optimizers.legacy` is not supported in Keras 3. \n",
    "#When using `tf.keras`, to continue using a `tf.keras.optimizers.legacy`\n",
    "#optimizer, you can install the `tf_keras` package (Keras 2) and set the environment variable\n",
    "#TF_USE_LEGACY_KERAS=True` to configure TensorFlow to use `tf_keras` when accessing `tf.keras`.\n",
    "\n",
    "import tf_keras as keras\n",
    "tfk = keras\n",
    "tfk.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f9bac-ada4-4c88-9b18-afea5e6a69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac88be1-f14a-4b79-9413-d6fcc8ed2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*')  # suppress error messages\n",
    "\n",
    "import math # just for train/test split\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875be94-d7e0-441e-bc19-5f9bf83b8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "  #data = df['len'].values.reshape((-1,1))\n",
    "  data = df[['QED', 'len']]#.values.reshape((-1,1)) # to get multiple column context\n",
    "  return data\n",
    "\n",
    "def preprocess_smiles_NaN(smiles):\n",
    "    global counter_failed\n",
    "    try:\n",
    "        encoded = sf.encoder(smiles)\n",
    "    except:\n",
    "        counter_failed =+ 1\n",
    "        encoded = 'nop'    # substitute empty strings for [nop]\n",
    "    return encoded\n",
    "\n",
    "def preprocess_smiles(smiles):\n",
    "    encoded = sf.encoder(smiles)\n",
    "    return encoded\n",
    "\n",
    "def keys_int(symbol_to_int):\n",
    "  d={}\n",
    "  i=0\n",
    "  for key in symbol_to_int.keys():\n",
    "    d[i]=key\n",
    "    i+=1\n",
    "  return d\n",
    "\n",
    "def get_selfies(df):\n",
    "    global selfies_list, largest_selfie_len, int_mol, selfies_alphabet, onehots, input_tensor, noise_tensor\n",
    "\n",
    "    sf.set_semantic_constraints()  # reset constraints\n",
    "    constraints = sf.get_semantic_constraints()\n",
    "    constraints['?'] = 3\n",
    "    sf.set_semantic_constraints(constraints)\n",
    "    \n",
    "    selfies_list = np.asanyarray(df.selfies)\n",
    "    selfies_alphabet = sf.get_alphabet_from_selfies(selfies_list)\n",
    "    # selfies_alphabet.add('.')  # test this\n",
    "    selfies_alphabet.add('[nop]')  # Add the \"no operation\" symbol as a padding character; selfies_alphabet.remove('[P]')\n",
    "    selfies_alphabet = list(sorted(selfies_alphabet))\n",
    "    # selfies_list = np.asanyarray(df.selfies) # added this line for a subset only if I want to load more selfie alphabet examples\n",
    "    largest_selfie_len = 40 #max(sf.len_selfies(s) for s in selfies_list)\n",
    "    symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "    int_mol=keys_int(symbol_to_int)\n",
    "    \n",
    "    onehots=sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int, largest_selfie_len)\n",
    "    input_tensor = tf.convert_to_tensor(onehots, dtype='float32')\n",
    "    noise_tensor = tf.random.uniform(shape=input_tensor.shape, minval=0, maxval=1, dtype='float32')\n",
    "    data = tf.add(input_tensor, noise_tensor) # dequantized data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa262ab-ce80-4d99-a458-74f97689ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../Dataset_after_cycle_4-corrected.csv',delimiter=',').dropna(subset=['QED']).drop_duplicates(subset=['Original_SMILES'])\n",
    "#data = df[df[\"Origin\"] == 'Initial'] #df.copy()\n",
    "data = pd.read_csv('data_cycles-40.csv',delimiter=',').dropna(subset=['QED']).drop_duplicates(subset=['Original_SMILES'])\n",
    "data['len'] = data['selfies'].apply(lambda x: sf.len_selfies(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3b57d-e697-438c-8522-13552b50e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.float32(get_selfies(data))\n",
    "y = np.float32(get_data(data))\n",
    "\n",
    "index = math.floor(x.shape[0]*1.0)\n",
    "\n",
    "x_train = x[:index]\n",
    "y_train = y[:index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8d74e-9e09-4010-a7f6-6cc7d8948745",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.convert_to_tensor(onehots, dtype='float64')\n",
    "noise_tensor = tf.random.uniform(shape=input_tensor.shape, minval=0, maxval=1, dtype='float64')\n",
    "dequantized_data = tf.add(input_tensor, noise_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff650a3-e751-45c0-a32d-689b357de2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NumpyDataset(dequantized_data)  # Create a DeepChem dataset\n",
    "splitter = RandomSplitter()\n",
    "train, val, test = splitter.train_valid_test_split(dataset=ds, seed=42)\n",
    "train_idx, val_idx, test_idx = splitter.split(dataset=ds, seed=42)\n",
    "\n",
    "dim = len(train.X[0])  # length of one-hot encoded vectors\n",
    "train.X.shape  # 459 samples, N-dimensional one-hot vectors that represent molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec23a0c-ba27-43f7-998a-b612ef5851c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=np.zeros(dim), scale_diag=np.ones(dim))\n",
    "\n",
    "if dim % 2 == 0:\n",
    "    permutation = tf.cast(np.concatenate((np.arange(dim / 2, dim), np.arange(0, dim / 2))),\n",
    "                                  tf.int32)\n",
    "else:\n",
    "    permutation = tf.cast(np.concatenate((np.arange(dim / 2 + 1, dim), np.arange(0, dim / 2))), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfc1d4-f277-4b08-a5bf-afe7b298d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8 # was 8\n",
    "flow_layers = []\n",
    "\n",
    "Made = tfb.AutoregressiveNetwork(params=2,\n",
    "                                 hidden_units=[32], activation='relu') # was 512, 512\n",
    "\n",
    "for i in range(num_layers):\n",
    "    flow_layers.append(        \n",
    "        (tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn=Made)\n",
    "    ))\n",
    "\n",
    "    permutation = tf.cast(np.random.permutation(np.arange(0, dim)), tf.int32)\n",
    "    \n",
    "    flow_layers.append(tfb.Permute(permutation=permutation))\n",
    "    \n",
    "#     if (i + 1) % int(2) == 0:\n",
    "#         flow_layers.append(tfb.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69146a12-e274-4384-a83c-855930bea2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nf = NormalizingFlow(base_distribution=base_dist,\n",
    "                    flow_layers=flow_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb03fcb-ccf6-4c0c-b69d-86bfac8131d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "nfm = NormalizingFlowModel(nf, learning_rate=1e-4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1a1d9-9b0a-4103-90d3-669b5a21e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_epochs = 600 # was 10\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  loss = nfm.fit(train, nb_epoch=1, all_losses=losses)\n",
    "  val_loss = nfm.create_nll(val.X)\n",
    "  val_losses.append(val_loss.numpy()) # negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a96ffb-76f9-488c-a85a-b14bc350ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(range(len(losses)), losses, label='train loss')\n",
    "ax.scatter(range(len(val_losses)), val_losses, label='val loss')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2954a-3cf0-49be-840e-351eec4433e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_samples = 50\n",
    "samples_transformed = np.empty((0,dim))#, float64)\n",
    "counter = 0\n",
    "\n",
    "while counter < 40:\n",
    "    print(counter)\n",
    "    counter += 1\n",
    "    generated_samples = nfm.flow.sample(num_samples)  # generative modeling\n",
    "    log_probs = nfm.flow.log_prob(generated_samples)  # probability density estimation\n",
    "    samples_transformed = np.concatenate((samples_transformed, generated_samples), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c8fe5-fb1e-4963-af42-9122a9956874",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c8b9e-b2b0-4a3e-8308-a1f2f077c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = tf.math.floor(samples_transformed)  # quantize data\n",
    "mols = tf.clip_by_value(mols, 0, 1)  # Set negative values to 0 and values > 1 to 1\n",
    "mols_list = mols.numpy().tolist()\n",
    "import time \n",
    "\n",
    "# Add padding characters if needed\n",
    "for mol in mols_list:\n",
    "    for i in range(largest_selfie_len):\n",
    "        row = mol[len(selfies_alphabet) * i: len(selfies_alphabet) * (i + 1)]\n",
    "    \n",
    "        for element in enumerate(row):\n",
    "            if np.isnan(element).any():\n",
    "                mol[len(selfies_alphabet) * (i+1) - 1] = 1\n",
    "            else:\n",
    "                if all(elem == 0 for elem in row):\n",
    "                    mol[len(selfies_alphabet) * (i+1) - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ea34a-ba46-499f-a445-8fe488a81f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols=sf.batch_flat_hot_to_selfies(mols_list, int_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046d4ba-1c93-468b-8a7f-9905f5429ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "valid_selfies, invalid_selfies = [], []\n",
    "for idx, selfies in enumerate(mols):\n",
    "  try:\n",
    "    if Chem.MolFromSmiles(sf.decoder(mols[idx]), sanitize=True) is not None:\n",
    "        valid_count += 1\n",
    "        valid_selfies.append(selfies)\n",
    "    else:\n",
    "      invalid_selfies.append(selfies)\n",
    "  except Exception:\n",
    "    pass\n",
    "print('%.2f' % (valid_count / len(mols)),  '% of generated samples are valid molecules.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846b6f5-7af0-4aac-bff6-595b8dd84aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mols = [Chem.MolFromSmiles(sf.decoder(vs)+'OP(C)(=O)F') for vs in valid_selfies]\n",
    "\n",
    "gen_mols = [i for i in gen_mols if i]\n",
    "\n",
    "smiles_generated = [sf.decoder(vs) for vs in valid_selfies] #+'OP(C)(=O)F' for vs in valid_selfies]\n",
    "\n",
    "from collections import OrderedDict\n",
    "OrderedDict((x, True) for x in smiles_generated).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b7e4c-9046-41f2-8761-1dd0cd57e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_range_min = data.idxmax()[0]+1\n",
    "\n",
    "df_generated = pd.DataFrame(smiles_generated, columns=[\"Original_SMILES\"])#.drop_duplicates()\n",
    "df_generated['len'] = df_generated['Original_SMILES'].apply(lambda x: len(x))\n",
    "counter_failed = 0\n",
    "df_generated['selfies'] = df_generated['Original_SMILES'].apply(preprocess_smiles_NaN)\n",
    "print(counter_failed)\n",
    "df_generated['RowID'] = pd.Series(samples_transformed.shape[0])\n",
    "df_generated['Origin'] = iteration\n",
    "df_generated.index += my_range_min\n",
    "new = pd.concat([data, df_generated])#.drop_duplicates(subset=['Original_SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c1fa9-860a-4cb2-b474-ca5263320a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new.iterrows():\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(row['Original_SMILES']+'OP(C)(=O)F')\n",
    "        qed = QED.default(mol)\n",
    "        try:\n",
    "            sas_score = sascorer.calculateScore(mol)\n",
    "        except:\n",
    "            sas_score = np.nan \n",
    "    except:\n",
    "        sas_score = np.nan\n",
    "        qed = np.nan\n",
    "    \n",
    "    new.at[index, \"QED\"] = qed\n",
    "    new.at[index, \"SA_score\"] = sas_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccd1ac-808f-4e91-ab01-161de287fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['SELFIES_Length'] = new['selfies'].apply(lambda x: sf.len_selfies(x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#ax.hist(new_vis['SELFIES_Length'], 15, density=True, histtype='step')#, stepped=True)\n",
    "ax = sns.histplot(data=new, x=\"SELFIES_Length\", hue=\"Origin\", element=\"step\", bins = 20, multiple=\"stack\")\n",
    "ax.set_xlabel('Selfie String Length')\n",
    "plt.axvline(x=16.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ed2d1-62d5-4252-8d08-95c26d25ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#ax.hist(new_vis['SELFIES_Length'], 15, density=True, histtype='step')#, stepped=True)\n",
    "ax = sns.histplot(data=new, x=\"QED\", hue=\"Origin\", element=\"step\", bins = 20, multiple=\"stack\")\n",
    "ax.set_xlabel('QED')\n",
    "plt.axvline(x=0.712277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d92db-bbf1-497a-8506-bddfe59e150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "new_altered = new.copy()\n",
    "new_altered['Origin'].replace({'1 Iter': '1-5 Iter', \n",
    "                               '2 Iter': '1-5 Iter',\n",
    "                               '3 Iter': '1-5 Iter',\n",
    "                               '4 Iter': '1-5 Iter',\n",
    "                               '5 Iter': '1-5 Iter',\n",
    "                               '6 Iter': '6-10 Iter',\n",
    "                               '7 Iter': '6-10 Iter',\n",
    "                               '8 Iter': '6-10 Iter',\n",
    "                               '9 Iter': '6-10 Iter',\n",
    "                               '10 Iter': '6-10 Iter',\n",
    "                              }, inplace=True)\n",
    "\n",
    "sns.jointplot(data=new_altered, x=\"QED\", y=\"SELFIES_Length\", hue=\"Origin\")#, xlim=(0,0.8), ylim=(0,30))\n",
    "sns.reset_orig\n",
    "plt.show()\n",
    "#ax.set_xlabel('Calculated QED for the sample molecule')\n",
    "#ax.set_ylim(bottom=0.1, top=0.80)\n",
    "#ax.set_xlim(left=0.1, right=0.80)\n",
    "#ax.legend(bbox_to_anchor=(1.0, 1.00))\n",
    "#plt.axvline(x=0, y=1)\n",
    "#plt.axhline(y=0.712277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a795e6-39be-4e29-9a1e-1fa865511a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "\n",
    "analysis_input=sf.batch_selfies_to_flat_hot(selfies_list, symbol_to_int, largest_selfie_len)\n",
    "analysis_input_results = np.zeros((len(analysis_input[0]),), dtype=int).tolist()\n",
    "\n",
    "for i in analysis_input: #range(0, 2):#len(analysis_input)):\n",
    "    #print(i)\n",
    "    summa = []\n",
    "    for k in range(0, len(analysis_input[0])):\n",
    "        #print(analysis_input_results[k] , 'and',  i[k])\n",
    "        summa.append(analysis_input_results[k] + i[k])\n",
    "    analysis_input_results = summa\n",
    "\n",
    "def split_list(lst, chunk_size):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "input_analysis = split_list(analysis_input_results, len(selfies_alphabet))\n",
    "# symbol_to_int # = x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b42bf-621a-4dad-80e6-f52786a0afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "\n",
    "analysis_output=sf.batch_selfies_to_flat_hot(mols, symbol_to_int, largest_selfie_len)\n",
    "analysis_output_results = np.zeros((len(analysis_output[0]),), dtype=int).tolist()\n",
    "\n",
    "for i in analysis_output: #range(0, 2):#len(analysis_input)):\n",
    "    #print(i)\n",
    "    summa = []\n",
    "    for k in range(0, len(analysis_output[0])):\n",
    "        summa.append(analysis_output_results[k] + i[k])\n",
    "    analysis_output_results = summa\n",
    "\n",
    "output_analysis = split_list(analysis_output_results, len(selfies_alphabet))\n",
    "\n",
    "histogram_inp = dict(list(enumerate(input_analysis)))\n",
    "histogram_out = dict(list(enumerate(output_analysis)))\n",
    "\n",
    "histogram_inp['Selfie'] = selfies_alphabet\n",
    "histogram_out['Selfie'] = selfies_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657c1c2-91c9-4ff2-8b9f-812b62cbb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize=(24, 30))\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.05, \n",
    "                    right=0.99, \n",
    "                    top=0.97,\n",
    "                    wspace=0.3, \n",
    "                    hspace=0.6\n",
    "                   )\n",
    "\n",
    "count=0\n",
    "position=[axes[0, 0], axes[0, 1], axes[0, 2], axes[0, 3],\n",
    "          axes[1, 0], axes[1, 1], axes[1, 2], axes[1, 3],\n",
    "          axes[2, 0], axes[2, 1], axes[2, 2], axes[2, 3],\n",
    "          axes[3, 0], axes[3, 1], axes[3, 2], axes[3, 3],\n",
    "          axes[4, 0], axes[4, 1], axes[4, 2], axes[4, 3],\n",
    "         ]\n",
    "\n",
    "names =     [\"0th\", \"1st\", \"2nd\", \"3rd\",\n",
    "             \"4th\", \"5th\", \"6th\", \"7th\",\n",
    "             \"8th\", \"9th\", \"10th\", \"11th\",\n",
    "             \"12th\", \"13th\", \"14th\", \"15th\",\n",
    "             \"16th\", \"17th\", \"18th\", \"19th\",\n",
    "            ]\n",
    "\n",
    "for ax, title in zip(axes.flat, names):\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "for i in range(0, len(position)):\n",
    "    data_vis = pd.DataFrame()\n",
    "    \n",
    "    data_vis['Selfie'] = selfies_alphabet\n",
    "    data_vis['Input'] = histogram_inp[i]\n",
    "    data_vis['Output'] = histogram_out[i]\n",
    "    \n",
    "    data_vis.plot(ax=position[count], kind='bar', x='Selfie', y=['Input', 'Output'], rot = 90)\n",
    "\n",
    "    position[count].set_yscale('log')\n",
    "#    position[count].set_ylim(top=200)\n",
    "    \n",
    "    # deleting the duplicate x and y labels\n",
    "    axes[0, 0].set_xlabel('', fontsize=20)\n",
    "    axes[0, 1].set_xlabel('', fontsize=20)\n",
    "    axes[0, 2].set_xlabel('', fontsize=20)\n",
    "    axes[0, 3].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 0].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 1].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 2].set_xlabel('', fontsize=20)\n",
    "#    axes[1, 3].set_xlabel('', fontsize=20)\n",
    "    axes[2, 0].set_xlabel('', fontsize=20)\n",
    "    axes[2, 1].set_xlabel('', fontsize=20)\n",
    "    axes[2, 2].set_xlabel('', fontsize=20)\n",
    "    axes[2, 3].set_xlabel('', fontsize=20)\n",
    "    \n",
    "    count=count+1\n",
    "\n",
    "fig.savefig('Distributions-in-out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d466293-a49f-4915-8230-ff4843ea45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = new.loc[(new['QED'] > 0.712277) | (new['SELFIES_Length'] > 10) ].loc[(new['Origin'] == iteration)].drop_duplicates(subset=['Original_SMILES']).dropna(subset=['QED'])\n",
    "saving = pd.concat([data, new_samples]).drop_duplicates(subset=['Original_SMILES']).dropna(subset=['QED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0ac7e-6fc5-4804-8d07-901f4e95aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901983e8-bf4c-4438-a289-6682bf49e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4737ba0-83bb-4688-b80d-909eca9f419d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b565a-4383-498a-a594-7d48d0128b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
