{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def2cbb-c175-41a7-b7ba-9e2b440c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle = 1\n",
    "seeding_dataset = 'data/ChemBL-35-cleaned.csv' # initialized with 50k-ChemBL.csv\n",
    "\n",
    "trainingset_path  = 'data/'+str(cycle)+'-training_set_org.csv'  \n",
    "charset_path= 'data/1-0.001-inp.h5'\n",
    "latent_dataset_path = 'data/'+str(cycle)+'-training_set_latent_space.csv'\n",
    "PATH = \"checkpoint_239.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49790172-1639-425f-83cd-968e204d68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from group_selfies import (\n",
    "    fragment_mols, \n",
    "    Group, \n",
    "    MolecularGraph, \n",
    "    GroupGrammar, \n",
    "    group_encoder\n",
    ")\n",
    "\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "import IPython.display # from ... import display\n",
    "from test_utils import *\n",
    "from rdkit import RDLogger\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "import os\n",
    "import sys\n",
    "from rdkit.Chem import RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "import sascorer\n",
    "from rdkit.Chem import QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52497b9-2d30-4e28-81ba-ecdc223ffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import gzip\n",
    "import pandas\n",
    "import h5py\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import h5py\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "def one_hot_array(i, n):\n",
    "    return map(int, [ix == i for ix in xrange(n)])\n",
    "\n",
    "def one_hot_index(vec, charset):\n",
    "    return map(charset.index, vec)\n",
    "\n",
    "def from_one_hot_array(vec):\n",
    "    oh = np.where(vec == 1)\n",
    "    if oh[0].shape == (0, ):\n",
    "        return None\n",
    "    return int(oh[0][0])\n",
    "\n",
    "def decode_smiles_from_indexes(vec, charset):\n",
    "    # Ensure that each element in 'vec' is a string (not numpy.bytes_)\n",
    "    return \"\".join(map(lambda x: str(charset[x], 'utf-8') if isinstance(charset[x], bytes) else charset[x], vec)).strip()\n",
    "\n",
    "def load_dataset(filename, split = True):\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "    if split:\n",
    "        data_train = h5f['data_train'][:]\n",
    "    else:\n",
    "        data_train = None\n",
    "    data_test = h5f['data_test'][:]\n",
    "    charset =  h5f['charset'][:]\n",
    "    h5f.close()\n",
    "    if split:\n",
    "        return (data_train, data_test, charset)\n",
    "    else:\n",
    "        return (data_test, charset)\n",
    "\n",
    "class MolecularVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(120, 9, kernel_size=9)\n",
    "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "        self.linear_0 = nn.Linear(280, 435) # changed from 70 to 280 to reflect the change of charset size\n",
    "        self.linear_1 = nn.Linear(435, 292)\n",
    "        self.linear_2 = nn.Linear(435, 292)\n",
    "        \n",
    "        self.linear_3 = nn.Linear(292, 292)\n",
    "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(501, 54) # changed this output from 33 to 54 to reflect the larger charset size\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "        return self.linear_1(x), self.linear_2(x)\n",
    "\n",
    "    def sampling(self, z_mean, z_logvar):\n",
    "        epsilon = 1e-2 * torch.randn_like(z_logvar)\n",
    "        return torch.exp(0.5 * z_logvar) * epsilon + z_mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)\n",
    "        output, hn = self.gru(z)\n",
    "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.sampling(z_mean, z_logvar)\n",
    "        return self.decode(z), z_mean, z_logvar\n",
    "\n",
    "\n",
    "def load_dataset_chunked(filename, split=True, batch_size=10000):\n",
    "    # Open the HDF5 file explicitly\n",
    "    h5f = h5py.File(filename, 'r')\n",
    "\n",
    "    # Memory-mapping the data (this avoids loading the entire dataset into memory at once)\n",
    "    data_test = np.array(h5f['data_test'], dtype='float32', copy=False)\n",
    "    \n",
    "    # Handle charset as strings directly\n",
    "    charset = h5f['charset']\n",
    "    if charset.dtype.kind in {'S', 'O'}:  # If it's a string or object type\n",
    "        charset = [x.decode('utf-8') if isinstance(x, bytes) else x for x in charset]  # Decode bytes if needed\n",
    "    else:\n",
    "        charset = np.array(charset, dtype='float32', copy=False)\n",
    "    \n",
    "    if split:\n",
    "        # Instead of loading the entire data_train, we'll iterate in chunks\n",
    "        data_train = h5f['data_train']\n",
    "        total_samples = data_train.shape[0]\n",
    "        \n",
    "        # Define the generator that reads data in chunks\n",
    "        def data_batch_generator():\n",
    "            \"\"\"Generator to load data in batches.\"\"\"\n",
    "            for i in range(0, total_samples, batch_size):\n",
    "                batch = data_train[i:i+batch_size]  # Read a batch from disk\n",
    "                yield batch\n",
    "\n",
    "        # Return the generator, data_test, and charset\n",
    "        return (data_batch_generator(), data_test, charset)\n",
    "    else:\n",
    "        # If not splitting, return data_test and charset only\n",
    "        return (data_test, charset)\n",
    "    \n",
    "    # Don't forget to close the file manually when done\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a3889-2895-45fb-90a6-5f5ee37ee453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MolecularVAE().to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class dotdict(dict): \n",
    "  __getattr__ = dict.get\n",
    "  __setattr__ = dict.__setitem__\n",
    "  __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c60fcd-e481-4183-94a6-a5f14da4f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_losses_p = checkpoint['train_losses_p']\n",
    "val_losses_p = checkpoint['val_losses_p']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a45ac-9021-4a1f-9563-3b5e110293df",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_data = pd.read_csv(latent_dataset_path)\n",
    "_, _, charset = load_dataset(charset_path)\n",
    "training_set_org = pd.read_csv(trainingset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f9c7b-c099-4602-8933-7a48748e1e7b",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd9c46-1a78-47bc-9f5e-a33651406935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.uniform import Uniform\n",
    "#max_latent_values = latent_data.max(axis=0).values.tolist()\n",
    "#min_latent_values = latent_data.max(axis=0).values.tolist()\n",
    "#r1 = min(min_latent_values)\n",
    "#r2 = max(max_latent_values)\n",
    "\n",
    "def generating_samples(per_cycle, latent_dim):\n",
    "    if sample_type == 'randn':\n",
    "        z_mean = torch.randn(per_cycle, latent_dim).to(device) #mean 0 and variance 1\n",
    "    elif sample_type == 'uniform':\n",
    "    # Alternatively from uniform distribution\n",
    "        z_mean = torch.FloatTensor(per_cycle, latent_dim).uniform_(r1, r2).to(device)\n",
    "    elif sample_type == 'uniform_d':\n",
    "        z_mean = Uniform(r1, r2).sample((per_cycle,latent_dim)).to(device)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    new_data = model.decode(z_mean)\n",
    "    new_data_numpy = new_data.cpu().detach().numpy().reshape(per_cycle, -1)\n",
    "\n",
    "    return new_data_numpy\n",
    "\n",
    "def generating_samples_cycles(cycles, per_cycle, latent_dim):\n",
    "    amount = cycles * per_cycle\n",
    "    length = len(charset)*120\n",
    "    new_data_numpy = np.empty((amount, length))\n",
    "    for i in range(cycles):\n",
    "        # Generate new samples\n",
    "        tempi = generating_samples(per_cycle=per_cycle, latent_dim=latent_dim)\n",
    "        # Concatenate the new data into the pre-allocated array\n",
    "        new_data_numpy[i * per_cycle: (i + 1) * per_cycle] = tempi\n",
    "    \n",
    "    return new_data_numpy\n",
    "\n",
    "sample_type = 'randn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed281ce-cb80-46a6-afc7-df0c23ba4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "valid_smiles = []\n",
    "#print('min and max latent: ', r1, r2)\n",
    "\n",
    "for cycles in range(1, 101):\n",
    "    new_data_numpy=generating_samples_cycles(cycles=10, per_cycle=100, latent_dim=292)\n",
    "\n",
    "    all_smiles = []\n",
    "    for id, molecule in enumerate(new_data_numpy):\n",
    "        all_smiles.append(decode_smiles_from_indexes(molecule.reshape(1, 120, len(charset)).argmax(axis=2)[0], charset))\n",
    "\n",
    "    for smi in all_smiles:\n",
    "        m = Chem.MolFromSmiles(smi,sanitize=True)\n",
    "        if m is None:\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                Chem.SanitizeMol(m)\n",
    "                valid_smiles.append(smi)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if cycles%10 == 0:\n",
    "        print('%.2f' % (len(valid_smiles) / len(all_smiles) / cycles *100), '% of generated samples are valid samples, ', len(valid_smiles), ' out of: ', len(all_smiles)*(cycles), 'in: {0:2.2f} min'.format( (time.time()-time0)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08704b6d-4d47-4cb1-805d-bcf88a2b4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "unique_smiles = OrderedDict((x, True) for x in valid_smiles).keys()\n",
    "print('%.2f' % (len(unique_smiles) / len(valid_smiles)*100),  '% of generated valid samples are unique samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd995020-0435-4c71-98d4-ebbb36b6f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating QED/SAS for ', len(unique_smiles), ' molecules out of all samples')\n",
    "df_generated = pd.DataFrame(unique_smiles, columns=[\"Original_SMILES\"]).drop_duplicates(subset=['Original_SMILES'])\n",
    "for index, row in df_generated.iterrows():\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(row['Original_SMILES'])#+'OP(C)(=O)F')\n",
    "            qed = QED.default(mol)\n",
    "            try:\n",
    "                sas_score = sascorer.calculateScore(mol)\n",
    "            except:\n",
    "                sas_score = np.nan\n",
    "        except:\n",
    "            sas_score = np.nan\n",
    "            qed = np.nan\n",
    "        \n",
    "        df_generated.at[index, \"QED\"] = qed\n",
    "        df_generated.at[index, \"SA_score\"] = sas_score\n",
    "        df_generated.at[index, \"Origin\"] = 'random'\n",
    "\n",
    "new = df_generated.dropna(subset=['QED', 'SA_score']).sort_values(['QED'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b9494-731f-4a96-b34a-b82df000e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f16e8c-647e-4756-bb7b-e3ad16c961a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(new['QED'], kde=False, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67171614-eb47-49cc-ac08-975b5c6f9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv('random.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd9915-0c79-447e-86f7-89fee3e31c72",
   "metadata": {},
   "source": [
    "# Check encoded training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41aa42-2d09-46ab-aa9f-5cc3b42b3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "newkl = model.decode(torch.tensor(latent_data.to_numpy(), dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e692a9e-e0e3-49a9-a5e4-6512780066ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smiles = []\n",
    "valid_smiles = []\n",
    "invalid_smiles = []\n",
    "attempt = newkl.cpu().detach().numpy().reshape(newkl.shape[0], -1)\n",
    "attempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dd24a-a696-43b2-bbbf-982295e364f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for id, molecule in enumerate(attempt):\n",
    "        all_smiles.append(decode_smiles_from_indexes(molecule.reshape(1, 120, len(charset)).argmax(axis=2)[0], charset))\n",
    "\n",
    "    for smi in all_smiles:\n",
    "        m = Chem.MolFromSmiles(smi,sanitize=False)\n",
    "        if m is None:\n",
    "            invalid_smiles.append(smi)\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                Chem.SanitizeMol(m)\n",
    "                valid_smiles.append(smi)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print('%.2f' % (len(valid_smiles)), ' are all smiles encoded out of ', len(all_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429dfcba-7fe9-4df8-a201-a5d41ebbd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_smiles[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
